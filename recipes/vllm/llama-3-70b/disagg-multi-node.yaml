# Llama 3.3-70B Disaggregated Multi-Node with vLLM
# Based on dynamo exemplar: recipes/llama-3-70b/vllm/disagg-multi-node
# Configuration: 1 prefill (TP8) + 1 decode (TP8) on 16x H100 GPUs (2 nodes)

name: "llama3-70b-vllm-disagg-mn"

model:
  path: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
  container: "nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.8.0"
  precision: "fp8"

resources:
  gpu_type: "h100"
  gpus_per_node: 8
  # Multi-node disaggregated:
  # 1 prefill worker @ TP8 = 8 GPUs (1 node)
  # 1 decode worker @ TP8 = 8 GPUs (1 node)
  # Total: 16 GPUs across 2 nodes
  prefill_nodes: 1
  decode_nodes: 1
  prefill_workers: 1
  decode_workers: 1
  gpus_per_prefill: 8
  gpus_per_decode: 8

frontend:
  type: dynamo
  enable_multiple_frontends: false

backend:
  type: vllm
  connector: nixl

  prefill_environment:
    PYTHONUNBUFFERED: "1"

  decode_environment:
    PYTHONUNBUFFERED: "1"

  vllm_config:
    prefill:
      served-model-name: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
      tensor-parallel-size: 8
      data-parallel-size: 1
      gpu-memory-utilization: 0.90
      no-enable-prefix-caching: true
      block-size: 128

    decode:
      served-model-name: "RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic"
      tensor-parallel-size: 8
      data-parallel-size: 1
      gpu-memory-utilization: 0.90
      no-enable-prefix-caching: true
      block-size: 128

benchmark:
  type: "mooncake-router"
  mooncake_workload: "conversation"
  ttft_threshold_ms: 2000
  itl_threshold_ms: 25
